{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_digit(string):\n",
    "    if string == 'A':\n",
    "        return 1\n",
    "    elif string == 'T':\n",
    "        return 2\n",
    "    elif string == 'G':\n",
    "        return 3\n",
    "    elif string == 'C':\n",
    "        return 4\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = np.load('./training_data.npz',allow_pickle=True)\n",
    "# data from file A as training data & valid data\n",
    "# see \n",
    "# Y. Gao, X. Chen, H. Qiao, Y. Ke, and H. Qi, \n",
    "# “Low-bias manipu- lation of DNA oligo pool for robust data storage,” \n",
    "# ACS Synthetic Biology, vol. 9, no. 12, pp. 3344–3352, 2020, pMID: 33185422.\n",
    "# for original data.\n",
    "# './training_data.npz' is preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seq = file_data['data_seq']\n",
    "# data_seq is a list whose elements are DNA-SEQs like 'AACCGGTTTAGCGT....'.\n",
    "label_list = file_data['label_list']\n",
    "# label_list is a list whose elements are list of COPY ERROR LABELS, for instance, \n",
    "# [[0,0,1,0,0,...],[0,1,0,0,...]] indicate\n",
    "# two kinds of copies with errors at position 2 and 1\n",
    "freq_list = file_data['freq_list']\n",
    "# freq_list is a list whose elements are list of frequencies of COPIES, corresponds to label_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "training_label = []\n",
    "for dna_seq, labels, copy_freq_list in zip(data_seq,label_list,freq_list):\n",
    "    xx_label = np.zeros_like(labels[0])\n",
    "    for a,b in zip(labels,copy_freq_list):\n",
    "        xx_label += (np.array(a)==1) * b\n",
    "    xx_label = xx_label/np.sum(copy_freq_list)\n",
    "    training_label.append(xx_label)\n",
    "    training_data.append([map_to_digit(letter) for letter in dna_seq[5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(training_label[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_data)\n",
    "training_label = (np.array(training_label)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_attention(states, name=0):\n",
    "    QUERY_KEY_DIMENTION = 16\n",
    "    VALUE_DIMENTION = 16\n",
    "    values = Dense(VALUE_DIMENTION,use_bias=False,activation='relu',kernel_regularizer='l2',name='values_dense_{}'.format(name))(states)\n",
    "    queries = Dense(QUERY_KEY_DIMENTION,use_bias=False,activation='relu',kernel_regularizer='l2',name='queries_dense_{}'.format(name))(states)\n",
    "    keys = Dense(QUERY_KEY_DIMENTION,use_bias=False,activation='relu',kernel_regularizer='l2',name='keys_dense_{}'.format(name))(states)\n",
    "    weights = dot([keys,queries],[2,2],name='weights_{}'.format(name))\n",
    "    weights = weights/np.sqrt(SEQ_LENGTH)\n",
    "    weights = Activation('softmax',name='attention_weights_{}'.format(name))(weights)\n",
    "    attention = dot([weights,values],[2,1])\n",
    "    print(attention)\n",
    "    attention = Dropout(0.3)(attention)\n",
    "    return attention\n",
    "def local_attention(states, name=0):\n",
    "    QUERY_KEY_DIMENTION = 32\n",
    "    VALUE_DIMENTION = 16\n",
    "    values = Dense(VALUE_DIMENTION,use_bias=False,activation='relu',kernel_regularizer='l2',name='local_values_dense_{}'.format(name))(states)\n",
    "    attention = Conv1D(16,kernel_size=3,strides=1,\n",
    "                       padding='same',activation='relu',kernel_regularizer='l2',name='local_conv1_{}'.format(name))(values)\n",
    "    attention = Conv1D(16,kernel_size=3,strides=1,\n",
    "                       padding='same',activation='relu',kernel_regularizer='l2',name='local_conv2_{}'.format(name))(attention)\n",
    "    attention = Dropout(0.3)(attention)\n",
    "    print(attention)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "num_encoder_tokens = 1\n",
    "num_decoder_tokens = 4\n",
    "SEQ_LENGTH = 15\n",
    "NUMBER_HEADS = 5\n",
    "NUMBER_LOCAL_HEADS = 5\n",
    "ATTENTION_MODEL_NUM = 1\n",
    "SEED = 0\n",
    "\n",
    "encoder_inputs = Input(shape=(SEQ_LENGTH, num_encoder_tokens),name='seq_input')\n",
    "c1 = Conv1D(filters=32,kernel_size=1,strides=1,padding='same',activation='relu',kernel_regularizer='l2')(encoder_inputs)\n",
    "c2 = Conv1D(filters=32,kernel_size=2,strides=1,padding='same',activation='relu')(encoder_inputs)\n",
    "c3 = Conv1D(filters=32,kernel_size=3,strides=1,padding='same',activation='relu',kernel_regularizer='l2')(encoder_inputs)\n",
    "c4 = Conv1D(filters=32,kernel_size=4,strides=1,padding='same',activation='relu')(encoder_inputs)\n",
    "c5 = Conv1D(filters=32,kernel_size=5,strides=1,padding='same',activation='relu',kernel_regularizer='l2')(encoder_inputs)\n",
    "source_hidden_states = concatenate([c1,c2,c3,c4,c5],axis=-1)\n",
    "\n",
    "multi_head = source_hidden_states\n",
    "print(multi_head)\n",
    "\n",
    "for L in range(ATTENTION_MODEL_NUM):\n",
    "    attentions = []\n",
    "    multi_head_saver = multi_head\n",
    "    for _ in range(NUMBER_HEADS):\n",
    "        attentions.append(global_attention(multi_head,name=_+L*100))\n",
    "    for _ in range(NUMBER_LOCAL_HEADS):\n",
    "        attentions.append(local_attention(multi_head,name=_+L*100))\n",
    "\n",
    "    multi_head = concatenate(attentions,axis=-1)\n",
    "    print(multi_head)\n",
    "    multi_head = Add()([multi_head,multi_head_saver])\n",
    "    multi_head = BatchNormalization()(multi_head)\n",
    "    multi_head = Dense(64,activation='relu')(multi_head)\n",
    "    multi_head = Dense(32,activation='relu')(multi_head)\n",
    "    print(multi_head)\n",
    "\n",
    "print(multi_head)\n",
    "\n",
    "\n",
    "outputs = Flatten()(multi_head)\n",
    "\n",
    "print(outputs)\n",
    "outputs = Dense(64,activation='relu',kernel_regularizer='l2')(outputs)\n",
    "outputs = Dense(32,activation='relu',kernel_regularizer='l2')(outputs)\n",
    "outputs = Dropout(0.3)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(2,activation='softmax')(outputs)\n",
    "print(outputs)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recall_neg(y_true, y_pred):\n",
    "    thsh = 0.5\n",
    "    pred = K.cast(y_pred>thsh,'float32')\n",
    "    diff = y_true - pred\n",
    "    diff1 = K.cast(diff<K.epsilon(),'float32')\n",
    "    diff2 = K.cast(diff>-K.epsilon(),'float32')\n",
    "    diff = diff1*diff2\n",
    "    y_1_acc = K.sum((diff) * y_true) / (K.sum(y_true) + K.epsilon())\n",
    "    \n",
    "    y_true_flip = K.cast((y_true-0.8) < 0,'float32')\n",
    "    y_0_acc = K.sum((diff) * y_true_flip) / (K.sum(y_true_flip) + K.epsilon())\n",
    "    \n",
    "    return y_0_acc\n",
    "\n",
    "def recall_pos(y_true, y_pred):\n",
    "    thsh = 0.5\n",
    "    pred = K.cast(y_pred > thsh,'float32')\n",
    "    diff = y_true - pred\n",
    "    diff1 = K.cast(diff<K.epsilon(),'float32')  # (neg to pos) and (pos to pos)\n",
    "    diff2 = K.cast(diff>-K.epsilon(),'float32') # (pos to neg) and (neg to neg)\n",
    "    diff = diff1*diff2\n",
    "    y_1_acc = K.sum((diff) * y_true) / (K.sum(y_true) + K.epsilon())\n",
    "    \n",
    "    y_true_flip = K.cast((y_true-0.8) < 0,'float32')\n",
    "    y_0_acc = K.sum((diff) * y_true_flip) / (K.sum(y_true_flip) + K.epsilon())\n",
    "    \n",
    "    return y_1_acc\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "loss = tf.losses.categorical_crossentropy\n",
    "metrics = ['acc',recall_neg,recall_pos]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_data, train_label = training_data,training_label\n",
    "_train_data = np.array(train_data)[:,:,np.newaxis]\n",
    "_train_label = np.array(train_label)\n",
    "\n",
    "N = SEQ_LENGTH\n",
    "train_data_pos = []\n",
    "train_label_pos = []\n",
    "train_data_neg = []\n",
    "train_label_neg = []\n",
    "cnt = 0\n",
    "POS_THSH = 0.05\n",
    "np.random.seed(SEED)\n",
    "for _data,_label, in zip(_train_data,_train_label):\n",
    "    for _ in range(len(_data)-N):\n",
    "        if np.sum(_label[_+N//2-1:_+N//2+2]) <= 0:\n",
    "            train_data_neg.append(_data[_:_+N])\n",
    "            train_label_neg.append(0)\n",
    "        elif _label[_+N//2] >= POS_THSH:\n",
    "            cnt += 1\n",
    "            train_data_pos.append(_data[_:_+N])\n",
    "            train_label_pos.append(1)\n",
    "\n",
    "train_data_pos = np.array(train_data_pos)\n",
    "train_label_pos = np.array(train_label_pos)\n",
    "train_data_pos,train_label_pos = shuffle(train_data_pos,train_label_pos)\n",
    "valid_data_pos = train_data_pos[-1000:]\n",
    "valid_label_pos = train_label_pos[-1000:]\n",
    "train_data_pos = train_data_pos[:-1000]\n",
    "train_label_pos = train_label_pos[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_neg,train_label_neg = shuffle(train_data_neg,train_label_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = (len(train_data_neg)-len(valid_data_pos))//len(train_data_pos)//5\n",
    "\n",
    "print(M)\n",
    "train_data_combine = np.concatenate([train_data_pos]*(M)+[train_data_neg[:M*train_data_pos.shape[0]]])\n",
    "train_label_combine = np.concatenate([train_label_pos]*(M)+[train_label_neg[:M*train_label_pos.shape[0]]])\n",
    "\n",
    "valid_data = np.concatenate([valid_data_pos,train_data_neg[-valid_data_pos.shape[0]:]])\n",
    "valid_label = np.concatenate([valid_label_pos,train_label_neg[-valid_label_pos.shape[0]:]])\n",
    "\n",
    "valid_data,valid_label = shuffle(valid_data,valid_label)\n",
    "train_data_combine,train_label_combine = shuffle(train_data_combine,train_label_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_data_combine,\n",
    "                 to_categorical(train_label_combine),\n",
    "                 batch_size=256, \n",
    "                 validation_data=(valid_data,to_categorical(valid_label)), shuffle=True, \n",
    "                 epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data_test = np.load('./testing_data.npz',allow_pickle=True)\n",
    "# Prepare testing data. \n",
    "# data from file B as testing data\n",
    "# see \n",
    "# Y. Gao, X. Chen, H. Qiao, Y. Ke, and H. Qi, \n",
    "# “Low-bias manipu- lation of DNA oligo pool for robust data storage,” \n",
    "# ACS Synthetic Biology, vol. 9, no. 12, pp. 3344–3352, 2020, pMID: 33185422.\n",
    "# for original data.\n",
    "# './testing_data.npz' is preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seq_test = file_data_test['data_seq']\n",
    "label_list_test = file_data_test['label_list']\n",
    "freq_list_test = file_data_test['freq_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = []\n",
    "testing_label = []\n",
    "for dna_seq, labels, copy_freq_list in zip(data_seq_test,label_list_test,freq_list_test):\n",
    "    xx_label = np.zeros_like(labels[0])\n",
    "    for a,b in zip(labels,copy_freq_list):\n",
    "        xx_label += (np.array(a)==1) * b\n",
    "    xx_label = xx_label/np.sum(copy_freq_list)\n",
    "    testing_label.append(xx_label)\n",
    "    testing_data.append([map_to_digit(letter) for letter in dna_seq[5:]])\n",
    "    \n",
    "testing_data = np.array(testing_data).astype(np.float)\n",
    "testing_label = np.array(testing_label).astype(np.float)\n",
    "\n",
    "SEQ_LENGTH = SEQ_LENGTH\n",
    "SEED = SEED\n",
    "POS_THSH = POS_THSH\n",
    "VALID_NUM = 1000\n",
    "test_data_pos = []\n",
    "test_data_neg = []\n",
    "test_label_pos = []\n",
    "test_label_pos_float = []\n",
    "test_label_neg = []\n",
    "for data,label in zip(testing_data,testing_label):\n",
    "    for _ in range(len(data)-N):\n",
    "        if label[_+N//2] <= 0:\n",
    "            test_data_neg.append(data[_:_+N])\n",
    "            test_label_neg.append(0)\n",
    "        elif label[_+N//2] >= POS_THSH:\n",
    "            test_data_pos.append(data[_:_+N])\n",
    "            test_label_pos.append(1)\n",
    "            test_label_pos_float.append(label[_+N//2])\n",
    "test_data_pos = np.array(test_data_pos)[:,:,np.newaxis]\n",
    "test_data_neg = np.array(test_data_neg)[:,:,np.newaxis]\n",
    "test_label_pos_float = np.array(test_label_pos_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_ans = model.predict(test_data_pos,batch_size=512)\n",
    "negative_ans = model.predict(test_data_neg,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,threshold = roc_curve([1]*len(positive_ans)+[0]*len(negative_ans),\n",
    "                              np.concatenate([positive_ans[:,1],negative_ans[:,1]]))\n",
    "auc_class = auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)'%(auc_class))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
